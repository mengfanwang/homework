\documentclass[22pt]{article} 
\usepackage{geometry} 
\usepackage{float} 
\usepackage{graphicx}
\usepackage{caption}
\usepackage{subfigure}
\usepackage{amsmath}
\usepackage{array}
\usepackage{amsfonts,amssymb} %空心字符
\geometry{left=2.0cm,right=2.0cm,top=0.5cm,bottom=0.5cm}
	\author{Mengfan Wang} 
	\title{Matrix Theory Homework 3} 
\begin{document}
	\maketitle 
	\paragraph{1}
		\subparagraph{a} Sufficient condition: if $\mathbf{y} \in Ker(\mathbf{X}^T\mathbf{X})$, then $\mathbf{X}^T\mathbf{Xy} = \mathbf{0}$.
		\begin{align}
			\mathbf{y}^T\mathbf{X}^T\mathbf{Xy} = \mathbf{y}^T\mathbf{0} = 0\\
			(\mathbf{Xy})^T\mathbf{Xy} = 0\\
			\mathbf{Xy} = \mathbf{0}
		\end{align}

		So, $\mathbf{y} \in Ker(\mathbf{X})$.

		Necessary condition: if $\mathbf{y} \in Ker(\mathbf{X})$, then $\mathbf{Xy} = \mathbf{0}$.
		So, $\mathbf{X}^T\mathbf{Xy} = \mathbf{X}^T\mathbf{0} = \mathbf{0}$, $\mathbf{y} \in Ker(\mathbf{X}^T\mathbf{X})$.

		In conclusion, $Ker(\mathbf{X}^T\mathbf{X}) = Ker(\mathbf{X})$.

		\subparagraph{b} From part 1 we have $Ker(\mathbf{X}^T\mathbf{X}) = Ker(\mathbf{X})$, so $nullity(\mathbf{X}^T\mathbf{X}) = nullity(\mathbf{X})$. Because $\mathbf{X}$ is a transformation from $\mathbb{R}^n$ to $\mathbb{R}^m$ and $\mathbf{X}^T\mathbf{X}$ is a transformation from $\mathbb{R}^n$ to $\mathbb{R}^n$, we have $nullity(\mathbf{X}^T\mathbf{X})+rank(\mathbf{X}^T\mathbf{X}) = nullity(\mathbf{X}) + rank(\mathbf{X}) = n $. Therefore, $rank(\mathbf{X}^T\mathbf{X}) = rank(\mathbf{X})$.

   		Similarly, $rank(\mathbf{XX}^T) = rank(\mathbf{X}^T) = rank(\mathbf{X})$. So, $rank(\mathbf{XX}^T) = rank(\mathbf{X}^T\mathbf{X})$.

	\paragraph{2}
		\subparagraph{a}$\forall \mathbf{x} \in \mathbb{R}^{n}$, $\exists \mathbf{w} = \mathbf{Qx} \in Ran(\mathbf{Q})$. Because $Ran(\mathbf{P}) = Ran(\mathbf{Q})$, $\exists \mathbf{z}$ that $\mathbf{w} = \mathbf{Pz} = \mathbf{PPz} = \mathbf{Pw}$. So, $\mathbf{PQx} = \mathbf{Pw} = \mathbf{w} = \mathbf{Qx}$. Therefore, $(\mathbf{PQ}-\mathbf{Q})\mathbf{x} = \mathbf{0}\  \forall \mathbf{x} \in \mathbb{R}^n$. As a result, $\mathbf{PQ} = \mathbf{Q}$.

		\subparagraph{b}$\forall \mathbf{w} \in \mathbb{R}^{n}$ can be decomposed as $\mathbf{w} = \mathbf{x} + \mathbf{y}$, while $\mathbf{x} \in Ran(\mathbf{Q})$ and $\mathbf{y} \in Ker(\mathbf{Q})$. So $\mathbf{y} \in Ker(\mathbf{P})$, too. And $\exists \mathbf{z}$ let $\mathbf{x} = \mathbf{Qz} = \mathbf{QQz} = \mathbf{Qx}$. Then we have:
		\begin{align}
			\mathbf{Px} & = \mathbf{Px}\\
			\mathbf{PQx} & = \mathbf{Px} \\
			\mathbf{PQx} + \mathbf{PQy} & = \mathbf{Px} + \mathbf{Py} \ (\mathbf{PQy} = \mathbf{Py} = \mathbf{0})\\
			\mathbf{PQ(x+y)} & = \mathbf{P(x+y)}\\
			\mathbf{PQw} & = \mathbf{Pw}
		\end{align}
		Therefore, $(\mathbf{PQ}-\mathbf{P})\mathbf{w} = \mathbf{0}\  \forall \mathbf{w} \in \mathbb{R}^n$. As a result, $\mathbf{PQ} = \mathbf{P}$.


		\subparagraph{c}Because
		\begin{align}
		&\mathbf{M}(\mathbf{I}-\mathbf{PSQ})\\
		=&(\mathbf{I}+\mathbf{PSQ})(\mathbf{I}-\mathbf{PSQ})\\
		=&\mathbf{I} + \mathbf{PSQ} - \mathbf{PSQ} -\mathbf{PSQPSQ}\\
		=&\mathbf{I} - (\mathbf{I}-\mathbf{Q})\mathbf{SQ}(\mathbf{I}-\mathbf{Q})\mathbf{SQ}\\
		=&\mathbf{I} - (\mathbf{SQ} - \mathbf{QSQ})(\mathbf{SQ} - \mathbf{QSQ})\\
		=&\mathbf{I} - \mathbf{SQSQ} + \mathbf{SQ}^2\mathbf{SQ} +\mathbf{QSQSQ} - \mathbf{QSQ}^2\mathbf{SQ}\\
		=&\mathbf{I} - \mathbf{SQSQ} + \mathbf{SQSQ}+\mathbf{QSQSQ}- \mathbf{QSQSQ}\\
		=&\mathbf{I}\\
		\end{align}
		So, $\mathbf{M}$ is invertible and $\mathbf{M}^{-1}=\mathbf{I}-\mathbf{PSQ}$.

	\paragraph{3}
		\subparagraph{a} If $\mathbf{Q}$ is a projector onto $Ker(\mathbf{A})$, $\mathbf{Q}^2 = \mathbf{Q}$ and $Ran(\mathbf{Q}) = Ker(\mathbf{A})$.
		\begin{align}
			\mathbf{Q}^2 &= (\mathbf{I}-\mathbf{B}_R\mathbf{A})(\mathbf{I}-\mathbf{B}_R\mathbf{A})\\
			& = \mathbf{I} - 2\mathbf{B}_R\mathbf{A} + \mathbf{B}_R\mathbf{A}\mathbf{B}_R\mathbf{A}\\
			& = \mathbf{I} - \mathbf{B}_R\mathbf{A}\\
			& = \mathbf{Q}
		\end{align}

		$\forall \mathbf{z} \in Ran(\mathbf{Q})$, $\exists \mathbf{x}$ so that $\mathbf{Qx=z}$:
		\begin{align}
			(\mathbf{I} - \mathbf{B}_R\mathbf{A})\mathbf{x} & = \mathbf{z} \\
			\mathbf{A}(\mathbf{I} - \mathbf{B}_R\mathbf{A})\mathbf{x} & = \mathbf{Az} \\
			\mathbf{Az} & = \mathbf{Ax} - \mathbf{AB}_R\mathbf{Ax}\\
			\mathbf{Az} & = \mathbf{0}\\
		\end{align}
		So $\mathbf{z} \in Ker(\mathbf{A})$, and $Ran(\mathbf{Q}) \subset Ker(\mathbf{A})$.

		$\forall \mathbf{z} \in Ker(\mathbf{A})$, $\mathbf{Az=0}$. So $\mathbf{Qz} = (\mathbf{I} - \mathbf{B}_R\mathbf{A})\mathbf{z} = \mathbf{z}$. So $\mathbf{z} \in Ran(\mathbf{Q})$, and $Ker(\mathbf{A} )\subset Ran(\mathbf{Q})$.

		As a result, $Ker(\mathbf{A} = Ran(\mathbf{Q})$ and $\mathbf{Q}$ is a projector onto $Ker(\mathbf{A})$.

		\subparagraph{b} If $\mathbf{Q}$ is an orthogonal projector onto $Ker(\mathbf{A})$, $\mathbf{Q}^* = \mathbf{Q}$. Define $\langle \mathbf{u},\mathbf{Q}\mathbf{v} \rangle = \mathbf{u}^T\mathbf{Q}\mathbf{v}  = \langle  \mathbf{Q}^*\mathbf{u},\mathbf{v} \rangle = \mathbf{u}^T(\mathbf{Q}^*)^T\mathbf{v} = \mathbf{u}^T\mathbf{Q}^T\mathbf{v},\ \forall \mathbf{u,v} \in \mathbb{R}^n$. So:
		\begin{align}
			\mathbf{Q} & = \mathbf{Q}^T\\
			\mathbf{I} - \mathbf{B}_R\mathbf{A} & = \mathbf{I} - \mathbf{A}^T\mathbf{B}_R^T\\
			\mathbf{B}_R\mathbf{A} & = \mathbf{A}^T\mathbf{B}_R^T\\
			\mathbf{B}_R\mathbf{A}\mathbf{B}_R & = \mathbf{A}^T\mathbf{B}_R^T\mathbf{B}_R\\
			\mathbf{B}_R & = \mathbf{A}^T\mathbf{B}_R^T\mathbf{B}_R\\
			\mathbf{A}\mathbf{A}^T\mathbf{B}_R^T\mathbf{B}_R & = \mathbf{I}\\
			\mathbf{B}_R^T\mathbf{B}_R & = (\mathbf{A}\mathbf{A}^T)^{-1}\\
			\mathbf{B}_R & = \mathbf{A}^T\mathbf{B}_R^T\mathbf{B}_R = \mathbf{A}^T(\mathbf{A}\mathbf{A}^T)^{-1}
		\end{align}
		Because $\mathbf{A}\mathbf{A}^T$ is the Gram matrix of $\mathbf{A}^T$, and the column vectors of $\mathbf{A}^T$ is independent because $\mathbf{A}$ has right inverse. So $\mathbf{A}\mathbf{A}^T$ is invertible. As a result, while $ \widetilde{\mathbf{B}_R}  = \mathbf{A}^T(\mathbf{A}\mathbf{A}^T)^{-1}$, $\widetilde{\mathbf{Q}} = \mathbf{I} - \widetilde{\mathbf{B}_R}\mathbf{A} $  is an orthogonal projector onto $Ker(\mathbf{A})$.
 
	\paragraph{4}
		\subparagraph{a} 

		Firstly, I'm going to show $tr(\mathbf{AB}) = tr(\mathbf{BA})$ while $\mathbf{A}=[a_{ij}], \mathbf{B} = [b_{ij}] \in \mathbb{R}^{n\times n}$:
		\begin{equation}
			tr(\mathbf{AB}) = \sum\limits_{i=1}^{n}\sum\limits_{j=1}^{n} a_{ij}b_{ji} = \sum\limits_{j=1}^{n}\sum\limits_{i=1}^{n} b_{ji}a_{ij} = tr(\mathbf{BA}) 
		\end{equation}

		Then, I'll show $P^*(\mathbf{Z}) = \mathbf{A}^T\mathbf{Z}\mathbf{B}^T$,because:
		\begin{align}
			\langle \mathbf{Y},P(\mathbf{Z}) \rangle_F & = tr(\mathbf{Y}^T\mathbf{AZB}) = tr(\mathbf{BY}^T\mathbf{AZ})\\
			\langle P^*(\mathbf{Y}),\mathbf{Z} \rangle_F & = tr[(\mathbf{A}^T\mathbf{Y}\mathbf{B}^T)^T\mathbf{Z}] = tr(\mathbf{BY}^T\mathbf{AZ})=\langle \mathbf{Y},P(\mathbf{Z}) \rangle_F\\
		\end{align}

		So $P^*(\mathbf{Z}) = \mathbf{A}^T\mathbf{Z}\mathbf{B}^T$.

		\subparagraph{b} Suppose $rank(\mathcal{M}) = a$, the column vectors of $\hat{\mathbf{M}} \in \mathbb{R}^{m\times a}$ is one basis of $\mathcal{M}$.

		Suppose the column vectors of $\tilde{\mathbf{N}}$  is one basis of $\mathcal{N}$, and the column vectors of $\hat{\mathbf{N}} \in \mathbb{R}^{n\times b}$ is a basis of $\{\mathbf{x}|\mathbf{x}^T\tilde{\mathbf{N}} = \mathbf{0}\ \forall \mathbf{x}\in \mathbb{R}^{n}\}$, while $\hat{\mathbf{N}}^T \tilde{\mathbf{N}} = \mathbf{0}$. We have $Ker(\hat{\mathbf{N}}^T) = \mathcal{N}$, because for any vectors in $\mathcal{N}$ can be represented as $\mathbf{x} = \tilde{\mathbf{N}}\mathbf{y}$, and $\hat{\mathbf{N}}^T\mathbf{x} = \hat{\mathbf{N}}^T \tilde{\mathbf{N}}\mathbf{y} = \mathbf{0}$.

		Apply Gram-Schmidt process to $\hat{\mathbf{M}}$ and $\hat{\mathbf{N}}$ to get $\mathbf{M}$ and $\mathbf{N}$, whose column vectors are the orthonormal basis of $\mathcal{M}$ and $\mathcal{N}$. Thus, we have $\mathbf{M}^T\mathbf{M} = \mathbf{I}$ and $\mathbf{N}^T\mathbf{N} = \mathbf{I}$. Now show the conclusion: When $\mathbf{A = MM}^T$ and $\mathbf{B=NN}^T$, $\mathbf{P(Z)}$ is the solution operator for the best approximation problem.

		Proof: If $\mathbf{P(Z)}$ is the solution operator for the best approximation problem, three conditions need to be satisfied: $\mathbf{P}^2(\mathbf{Z}) = \mathbf{P(Z)}$, $\mathbf{P^*(Z)} = \mathbf{P(Z)}$, and $Ran(\mathbf{P}) = \mathcal{W}$.

		(1) \begin{align}
			\mathbf{P}^2(\mathbf{Z}) & = \mathbf{MM}^T(\mathbf{MM}^T\mathbf{ZNN}^T)\mathbf{NN}^T\\
			& = \mathbf{MM}^T\mathbf{ZNN}^T\\
			& = \mathbf{P(Z)}
		\end{align}

		(2) From part a we can know $P^*(\mathbf{Z}) = \mathbf{A}^T\mathbf{Z}\mathbf{B}^T = (\mathbf{MM}^T)^T\mathbf{Z}(\mathbf{NN}^T)^T=\mathbf{MM}^T\mathbf{ZNN}^T = \mathbf{P(Z)} $.

		(3) Suppose $\mathbf{M} = [\mathbf{m}_1\ \mathbf{m}_2\ \dots\ \mathbf{m}_a]$ and $\mathbf{N} = [\mathbf{n}_1\ \mathbf{n}_2\ \dots\ \mathbf{n}_b]$. Firstly, we need to prove $\{\mathbf{X}|\mathbf{X} = \mathbf{m}_i\mathbf{n}_j^T,\ 1\leq i \leq a\ and\ 1\leq j\leq b \}$ is a basis of $\mathcal{W}$.

		\quad (3.1)$\{\mathbf{X}|\mathbf{X} = \mathbf{m}_i\mathbf{n}_j^T\}$ is linearly independent. Because $\{\mathbf{m}_1,\mathbf{m}_2,\dots,\mathbf{m}_a\}$ is a basis of $\mathcal{M}$ and $\{\mathbf{n}_1,\mathbf{n}_2,\dots,\mathbf{n}_b\}$ is a basis of $\mathcal{N}$, we have: The only solution of $\sum\limits_{i=1}^a \alpha_i\mathbf{m}_i = \mathbf{0}$ is $\alpha_i = 0$ for all $i$ and the only solution of $\sum\limits_{i=1}^b \beta_i\mathbf{n}_i = \mathbf{0}$ is $\beta_i = 0$ for all $i$. As a result, the solution of $(\sum\limits_{i=1}^a \alpha_i\mathbf{m}_i)(\sum\limits_{j=1}^b \beta_j\mathbf{n}_j^T) = \mathbf{0}$ is $\alpha_i = 0$ for all $i$ or $\beta_j = 0$ for all $j$. If existing some $b_j \not= 0$: The only solution of $(\sum\limits_{i=1}^a \alpha_i\mathbf{m}_i)(\sum\limits_{j=1}^b \beta_j\mathbf{n}_j^T) = \sum\limits_{i=1}^a \mathbf{m}_i(\alpha_i\sum\limits_{j=1}^b \beta_j\mathbf{n}_j^T) = \mathbf{0}$ is $\alpha_i=0$ for all $i$ because $(\sum\limits_{j=1}^b \beta_j\mathbf{n}_j^T) \not = \mathbf{0}$ and vice versa. So the only solution of $(\sum\limits_{i=1}^a \alpha_i\mathbf{m}_i)(\sum\limits_{j=1}^b \beta_j\mathbf{n}_j^T)  = \sum\limits_{i=1}^a\sum\limits_{j=1}^b \alpha_i \beta_j\mathbf{m}_i\mathbf{n}_j^T$ is $\alpha_i \beta_j = 0$ for all $i,j$. $\{\mathbf{X}|\mathbf{X} = \mathbf{m}_i\mathbf{n}_j^T\}$ is linearly independent.

		\quad (3.2) $span(\{\mathbf{X}|\mathbf{X} = \mathbf{m}_i\mathbf{n}_j^T\}) = \mathcal{W}$. $\forall \hat{\mathbf{X}} \in span(\{\mathbf{X}|\mathbf{X} = \mathbf{m}_i\mathbf{n}_j^T\})  $ can be represented as a linear combination of the bases: 
		\begin{align}	
		\hat{\mathbf{X}} & = \sum\limits_{i=1}^a\sum\limits_{j=1}^b \lambda_{ij}\mathbf{m}_i\mathbf{n}_j^T\\
		& =  \sum\limits_{i=1}^a\sum\limits_{j=1}^b \lambda_{ij}\mathbf{Me}_i(\mathbf{Ne}_j)^T\\
		& = \mathbf{M}(\sum\limits_{i=1}^a\sum\limits_{j=1}^b \lambda_{ij}\mathbf{e}_i\mathbf{e}_j^T)\mathbf{N}^T
		\end{align}
		So, $Ran(\hat{\mathbf{X}}) = Ran(\mathbf{M}(\sum\limits_{i=1}^a\sum\limits_{j=1}^b \lambda_{ij}\mathbf{e}_i\mathbf{e}_j^T)\mathbf{N}^T) \subset Ran(\mathbf{M})= Ran(\mathbf{MM}^T) = \mathcal{M}$. And $\forall \mathbf{z} \in \mathcal{N} = Ker(\mathbf{NN}^T) = Ker(\mathbf{N}^T)$, $\hat{\mathbf{X}}\mathbf{z} = \mathbf{M}(\sum\limits_{i=1}^a\sum\limits_{j=1}^b \lambda_{ij}\mathbf{e}_i\mathbf{e}_j^T)\mathbf{N}^T=\mathbf{0}$. Therefore, $\mathcal{N}\subset Ker(\hat{\mathbf{X}}) $. So $span(\{\mathbf{X}|\mathbf{X} = \mathbf{m}_i\mathbf{n}_j^T\}) = \mathcal{W}$.

	As a result, $\{\mathbf{X}|\mathbf{X} = \mathbf{m}_i\mathbf{n}_j^T \}$ is a basis of $\mathcal{W}$. Now show $\mathcal{W} =  Ran(\mathbf{P})$.

	(3.3) $\forall \hat{\mathbf{X}} \in \mathcal{W}$, it can be represented as $\hat{\mathbf{X}} = \mathbf{M}(\sum\limits_{i=1}^a\sum\limits_{j=1}^b \lambda_{ij}\mathbf{e}_i\mathbf{e}_j^T)\mathbf{N}^T $.
	\begin{align}
		\mathbf{P(\hat{\mathbf{X}})} & =  \mathbf{MM}^T\mathbf{M}(\sum\limits_{i=1}^a\sum\limits_{j=1}^b \lambda_{ij}\mathbf{e}_i\mathbf{e}_j^T)\mathbf{N}^T\mathbf{NN^T}\\
		& = \mathbf{M}(\sum\limits_{i=1}^a\sum\limits_{j=1}^b \lambda_{ij}\mathbf{e}_i\mathbf{e}_j^T)\mathbf{N}^T\\
		& = \hat{\mathbf{X}}
	\end{align}
	So $\hat{\mathbf{X}} \in Ran(\mathbf{P})$ and $\mathcal{W} \subset Ran(\mathbf{P})$.

	(3.4) If $\mathbf{X} \in  Ran(\mathbf{P})$, there must exist $\mathbf{Z}$ so that $\mathbf{P(Z)} = \mathbf{MM}^T\mathbf{ZNN}^T$. So, $Ran(\mathbf{X}) = Ran(\mathbf{MM}^T\mathbf{ZNN}^T)\subset Ran(\mathbf{M}) = \mathcal{M}$. And $\forall \mathbf{z} \in \mathcal{N}$ we have $\mathbf{N}^T\mathbf{z} = \mathbf{0}$, so $\mathbf{Xz} = \mathbf{MM}^T\mathbf{ZNN}^T\mathbf{z} = \mathbf{0}$. Therefore, $Ran(\mathbf{P}) \subset \mathcal{W}$. So $Ran(\mathbf{P}) =\mathcal{W}$.

	In conclusion, when $\mathbf{A = MM}^T$ and $\mathbf{B=NN}^T$, $\mathbf{P(Z)}$ is the solution operator for the best approximation problem.

	\paragraph{5} Suppose this matrix $\mathbf{A} = \mathbf{QR} = [\mathbf{a}_1\ \mathbf{a}_2\ \mathbf{a}_3]$, while $\mathbf{Q} = [\mathbf{q}_1\ \mathbf{q}_2\ \mathbf{q}_3]$ is orthonormal, $\mathbf{a}$ and $\mathbf{q}$ are column vectors. Applying the Gram-Schmidt process to $\{\mathbf{a}_1\ \mathbf{a}_2\ \mathbf{a}_3\}$:
	\begin{align}
		\mathbf{q}_1 & = \frac{\mathbf{a}_1}{\left\|\mathbf{a}_1\right\|} = [\frac{\sqrt{2}}{2}\ 0\ 0\ -\frac{\sqrt{2}}{2}]^T \\
		\mathbf{q}_2 & = \frac{\mathbf{a}_2- \left \langle\mathbf{q}_1,\mathbf{a}_2\right \rangle\mathbf{q}_1}{\left\|\mathbf{a}_2- \left \langle\mathbf{q}_1,\mathbf{a}_2\right \rangle\mathbf{q}_1\right\|} = [-\frac{\sqrt{6}}{6}\ \frac{\sqrt{6}}{3}\ 0\ -\frac{\sqrt{6}}{6}]^T\\
		\mathbf{q}_3 & = \frac{\mathbf{a}_3- \left \langle\mathbf{q}_1,\mathbf{a}_3\right \rangle\mathbf{q}_1-\left \langle\mathbf{q}_2,\mathbf{a}_3\right \rangle\mathbf{q}_2}{\left\|\mathbf{a}_3- \left \langle\mathbf{q}_1,\mathbf{a}_3\right \rangle\mathbf{q}_1-\left \langle\mathbf{q}_2,\mathbf{a}_3\right \rangle\mathbf{q}_2 \right\|} = [-\frac{\sqrt{3}}{6}\ -\frac{\sqrt{3}}{6}\ \frac{\sqrt{3}}{2}\ -\frac{\sqrt{3}}{6}]^T
		\end{align}
	\begin{equation}
	\renewcommand\arraystretch{1.5}
	\mathbf{Q}  = [\mathbf{q}_1\ \mathbf{q}_2\ \mathbf{q}_3] = \left[\begin{array}{ccc} \frac{\sqrt{2}}{2} & -\frac{\sqrt{6}}{6} & -\frac{\sqrt{3}}{6}\\ 0 & \frac{\sqrt{6}}{3} & -\frac{\sqrt{3}}{6} \\ 0&0&\frac{\sqrt{3}}{2}\\ -\frac{\sqrt{2}}{2}&-\frac{\sqrt{6}}{6}& -\frac{\sqrt{3}}{6} \end{array}\right]
		\end{equation}
		\begin{equation}
	\renewcommand\arraystretch{1.5}
	\mathbf{R} = \mathbf{Q}^{-1}\mathbf{A} = \mathbf{Q}^T\mathbf{A} = \left[\begin{array}{ccc} \sqrt{2} &\frac{\sqrt{2}}{2} & \frac{\sqrt{2}}{2} \\ 0 & \frac{\sqrt{6}}{2} & \frac{\sqrt{6}}{6} \\ 0 & 0 & \frac{2\sqrt{3}}{3}\\\end{array}\right]
		\end{equation}

	\paragraph{6}
		\subparagraph{a} Suppose there are two matrices $\mathbf{X}_1$ and $\mathbf{X}_2$ satisfy all four conditions, then we have:
		\begin{align}
			\mathbf{X}_1 & = \mathbf{X}_1\mathbf{AX}_1 = \mathbf{X}_1(\mathbf{AX}_1)^T = \mathbf{X}_1[(\mathbf{AX}_2\mathbf{A})\mathbf{X}_1]^T = \mathbf{X}(\mathbf{AX}_1)^T(\mathbf{AX}_2)^T\\& = \mathbf{X}_1\mathbf{AX}_1\mathbf{AX}_2 = \mathbf{X}_1\mathbf{AX}_2 = \mathbf{X}_1\mathbf{A}(\mathbf{X}_2\mathbf{A}\mathbf{X}_2) = (\mathbf{X}_1\mathbf{A})^T(\mathbf{X}_2\mathbf{A})^T\mathbf{X}_2\\ & = (\mathbf{X}_2\mathbf{AX}_1\mathbf{A})^T\mathbf{X}_2 = (\mathbf{X}_2\mathbf{A})^T\mathbf{X}_2 = \mathbf{X}_2\mathbf{AX}_2 = \mathbf{X}_2
		\end{align}
		So, there can be at most one matrix $\mathbf{X}$ that satisfies all four conditions.

		\subparagraph{b}Suppose $\mathbf{A}$ can be full-rank factorized as $\mathbf{MN}^T$, $\mathbf{A}^\dagger = \mathbf{N}(\mathbf{N}^T\mathbf{N})^{-1}(\mathbf{M}^T\mathbf{M})^{-1}\mathbf{M}^T$. So,
		\begin{align}
			\mathbf{AA}^\dagger\mathbf{A} & = \mathbf{MN}^T\mathbf{N}(\mathbf{N}^T\mathbf{N})^{-1}(\mathbf{M}^T\mathbf{M})^{-1}\mathbf{M}^T\mathbf{MN}^T\\
			& = \mathbf{MN}^T = \mathbf{A}\\
			\mathbf{A}^\dagger\mathbf{AA}^\dagger & = \mathbf{N}(\mathbf{N}^T\mathbf{N})^{-1}(\mathbf{M}^T\mathbf{M})^{-1}\mathbf{M}^T\mathbf{MN}^T\mathbf{N}(\mathbf{N}^T\mathbf{N})^{-1}(\mathbf{M}^T\mathbf{M})^{-1}\mathbf{M}^T\\
			& =  \mathbf{N}(\mathbf{N}^T\mathbf{N})^{-1}(\mathbf{M}^T\mathbf{M})^{-1}\mathbf{M}^T = \mathbf{A}^\dagger\\
			(\mathbf{AA}^\dagger)^T & = (\mathbf{MN}^T\mathbf{N}(\mathbf{N}^T\mathbf{N})^{-1}(\mathbf{M}^T\mathbf{M})^{-1}\mathbf{M}^T)^T\\
			& = (\mathbf{M}(\mathbf{M}^T\mathbf{M})^{-1}\mathbf{M}^T)^T\\
			& = \mathbf{M}(\mathbf{M}^T\mathbf{M})^{-1}\mathbf{M}^T\\
			& = \mathbf{MN}^T\mathbf{N}(\mathbf{N}^T\mathbf{N})^{-1}(\mathbf{M}^T\mathbf{M})^{-1}\mathbf{M}^T = \mathbf{AA}^\dagger\\
			(\mathbf{A}^\dagger\mathbf{A})^T & = (\mathbf{N}(\mathbf{N}^T\mathbf{N})^{-1}(\mathbf{M}^T\mathbf{M})^{-1}\mathbf{M}^T\mathbf{MN}^T)^T\\
			& = (\mathbf{N}(\mathbf{N}^T\mathbf{N})^{-1}\mathbf{N}^T)^T\\
			& = \mathbf{N}(\mathbf{N}^T\mathbf{N})^{-1}\mathbf{N}^T\\
			& = \mathbf{N}(\mathbf{N}^T\mathbf{N})^{-1}(\mathbf{M}^T\mathbf{M})^{-1}\mathbf{M}^T\mathbf{MN}^T = \mathbf{A}^\dagger\mathbf{A},
		\end{align}
		while $[(\mathbf{N}^T\mathbf{N})^{-1}]^T=[(\mathbf{N}^T\mathbf{N})^{T}]^{-1} = (\mathbf{N}^T\mathbf{N})^{-1}$ and $[(\mathbf{M}^T\mathbf{M})^{-1}]^T =[(\mathbf{M}^T\mathbf{M})^{T}]^{-1}= (\mathbf{M}^T\mathbf{M})^{-1}$.
		So $\mathbf{A}^\dagger$ satisfies all four conditions, and because there can be at most one matrix that satisfies all four conditions, these conditions can be used to characterize pseudoinverses.


	\paragraph{7}
		\subparagraph{a} Because $\mathbf{Q}$ has orthonormal columns, $\mathbf{Q}^T\mathbf{Q}$ is the Gram matrix of its columns and equals to $\mathbf{I}$. And $\mathbf{Q}$ is invertible, so can be full-rank factorized as $\mathbf{Q = QI}$. Then:
		\begin{align}
			\mathbf{Q}^\dagger & = \mathbf{I}(\mathbf{I}^T\mathbf{I})^{-1}(\mathbf{Q}^T\mathbf{Q})^{-1}\mathbf{Q}^T\\
			& = \mathbf{Q}^T
		\end{align}

		\subparagraph{b} Define the standard dot product as the inner product. Suppose $rank( \mathbf{P}) = p$, $ \mathbf{P}$ can be factorized as $ \mathbf{P} =  \mathbf{X} \mathbf{Y}^T$. Because $\mathbf{P}$ is an orthogonal projection, we have $\mathbf{P}^T = \mathbf{P}$. So, $\mathcal{S} = Ran(\mathbf{P})  = Ran(\mathbf{XY}^T) = Ran(\mathbf{X}) = Ran(\mathbf{YX}^T) = Ran(\mathbf{Y})$. Then:
		\begin{align}
			\mathbf{P} & = \mathbf{YX}^T\\
			\mathbf{PX} & = \mathbf{YX}^T\mathbf{X}\\
			\mathbf{Y} & = \mathbf{PX}(\mathbf{X}^T\mathbf{X})^{-1}
		\end{align}
		Because $\mathbf{Y} \subset Ran(\mathbf{Y}) = \mathcal{S}$, and $ \mathbf{X}(\mathbf{X}^T\mathbf{X})^{-1} \subset Ran(\mathbf{X}) = \mathcal{S}$, $\mathbf{Y}  = \mathbf{PX}(\mathbf{X}^T\mathbf{X})^{-1}  = \mathbf{X}(\mathbf{X}^T\mathbf{X})^{-1}$. Then: 
		\begin{align}
			\mathbf{P}^\dagger & = \mathbf{Y}(\mathbf{Y}^T\mathbf{Y})^{-1}(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\\
			& = \mathbf{X}(\mathbf{X}^T\mathbf{X})^{-1}[(\mathbf{X}(\mathbf{X}^T\mathbf{X})^{-1})^T\mathbf{X}(\mathbf{X}^T\mathbf{X})^{-1}]^{-1}(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\\
			& = \mathbf{X}(\mathbf{X}^T\mathbf{X})^{-1}(\mathbf{X}^T\mathbf{X})(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\\
			& = \mathbf{X}(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\\
			& = \mathbf{YX}^T\\
			& = \mathbf{P}
		\end{align}
		As a result, $\mathbf{P}^\dagger = \mathbf{P}$.
 
		\subparagraph{c}
		\begin{align}
			\mathbf{A}& = \mathbf{LU} = \left[ \begin{array}{ccc} 1&0&0\\2&1&0\\1&1&1\\ \end{array}\right] \left[ \begin{array}{ccc} 2&1&0\\0&1&1\\0&0&0\\\end{array}\right]\\
			& = \mathbf{XY}^T =  \left[ \begin{array}{cc} 1&0\\2&1\\1&1\\ \end{array}\right]\left[ \begin{array}{ccc} 2&1&0\\0&1&1\\\end{array}\right]
		\end{align}
		And,
		\begin{equation}
			(\mathbf{X}^T\mathbf{X})^{-1} = \left[ \begin{array}{cc}6&3\\3&2\\ \end{array}\right]^{-1} =  \left[ \begin{array}{ccc}2/3&-1\\ -1&2\\ \end{array}\right]	
		\end{equation}
		\begin{equation}
			(\mathbf{Y}^T\mathbf{Y})^{-1} = \left[ \begin{array}{cc}5&1\\1&2\\ \end{array}\right]^{-1} =  \left[ \begin{array}{ccc}2/9&-1/9\\ -1/9&5/9\\ \end{array}\right]
		\end{equation}
		So, 
		\begin{align}
			\mathbf{A}^\dagger & = \mathbf{Y}(\mathbf{Y}^T\mathbf{Y})^{-1}(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\\
			& = \left[ \begin{array}{ccc} 14/27& 4/27&-10/27\\ -10/27&1/27&11/27\\-17/27  &-1/27 &16/27\\     \end{array}\right]
		\end{align}

\end{document}
 \left[ \begin{array}{ccc} \end{array}\right]